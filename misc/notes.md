### Max batch sizes
on 24GB Ampere GPU:
mT5 600M - 8
NLLB 1B - 4

on 49GB Ampere GPU:
mT5 3B - 4
NLLB 3B - 16


### Ideas
- Activation function diversification
- Single layer model with many attention heads
